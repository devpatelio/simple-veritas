{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13889\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import urllib\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.onnx\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(DIRECTORY):\n",
    "    for name in files:\n",
    "        paths.append(os.path.join(root, name))\n",
    "\n",
    "names = [i.split('/')[-1] for i in paths][1:]\n",
    "data_dict = dict(zip([i[:-4] for i in names], paths[1:]))\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "        self.max_len = 600\n",
    "\n",
    "        self.x_data, self.token = self.word_vector(self.x_data)\n",
    "\n",
    "        self.data[x_col] = [torch.tensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        maximum_length = 600\n",
    "        t = Tokenizer(filters='\\n.,:!\"#$()&@%^()-_`~[];.,{|}')\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "\n",
    "        return sequences, t\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            data.loc[:, column] = [torch.tensor(ref[data.loc[idx, column]]) for idx in range(len(data))]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])\n",
    "token_basis = clean_truth_data.token\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = np.array(next(a)[0])\n",
    "inp_size = (b.shape)[1]\n",
    "\n",
    "\n",
    "import itertools\n",
    "ab = list(itertools.chain(*[i[0] for i in clean_truth_data]))\n",
    "ab = set([int(i) for i in ab])\n",
    "emb_dim = len(ab)\n",
    "print(emb_dim)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)        \n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        x = self.dropout(F.relu(self.fc7(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        print(hidden.shape)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 1\n",
    "\n",
    "\n",
    "def train(net, train_loader, LR, DECAY, EPOCHS):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    epochs = EPOCHS\n",
    "    losses = []\n",
    "\n",
    "    for step in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inp, labels = data\n",
    "            if net == recurrent:\n",
    "                inp, labels = inp.long().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            elif net == feedforward:\n",
    "                inp, labels = inp.float().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), labels)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += cost.item()\n",
    "        print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "    print('Training Complete')  \n",
    "\n",
    "    return losses\n",
    "\n",
    "def eval(net, test_loader):\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inp, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inp.float())\n",
    "        output = output.detach().numpy()\n",
    "        output = list(output)\n",
    "        output = [list(i).index(max(i)) for i in output]\n",
    "        \n",
    "        for idx, item in enumerate(torch.tensor(output)):\n",
    "            total += 1\n",
    "            if item == labels[idx]:\n",
    "                acc += 1\n",
    "    print(f'{acc/total*100}%')\n",
    "\n",
    "def format_raw_text(token):\n",
    "    token = token.replace(' ', 'uxd')\n",
    "    clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "    return clean_token\n",
    "\n",
    "def model_load(net, PATH, name, export=True):\n",
    "    if export:\n",
    "        torch.save(net.state_dict(), PATH+name+'.pth')\n",
    "        return PATH+name+'.pth'\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(PATH + name + '.pth', map_location=torch.device('cpu')))\n",
    "        net.eval()\n",
    "        return net\n",
    "\n",
    "def sentiment(inp_text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(inp_text)\n",
    "    \n",
    "def meta_extract(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.download('punkt')\n",
    "    article.nlp()\n",
    "    return article.authors, article.publish_date, article.top_image, article.images, article.title, article.summary\n",
    "\n",
    "def tokenize_sequence(text_inp, tokenizer):\n",
    "    text_inp = text_inp.lower().split('\\n')\n",
    "    tokenizer.fit_on_texts(text_inp)\n",
    "    sequences = tokenizer.texts_to_sequences(text_inp)\n",
    "    sequences = [i if i!=[] else [0] for i in tokenizer.texts_to_sequences(text_inp)]\n",
    "    sequences = [i[0] for i in sequences]\n",
    "    pad_len =  [0]*int(inp_size - len(sequences))\n",
    "    sequences += pad_len\n",
    "    return torch.FloatTensor(sequences)[:600]\n",
    "\n",
    "def prediction(inp, model):\n",
    "    output = model(inp)\n",
    "    return output\n",
    "\n",
    "# model_load(recurrent, 'model_parameters/', 'lstm_politifact')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=600, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc5): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc6): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (fc7): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "RecurrentClassifier(\n",
      "  (embedding): Embedding(13889, 600)\n",
      "  (rnn): LSTM(600, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feedforward = FeedForward(ref_check, inp_size).to(device)\n",
    "print(feedforward)\n",
    "recurrent = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)\n",
    "print(recurrent)\n",
    "\n",
    "# with open('serialized/recurrent_empty.pickle', 'wb') as f:\n",
    "#     recurrent = pickle.load(f)\n",
    "\n",
    "# with open('serialized/feedforward_empty.pickle', 'wb') as f:\n",
    "#     feedforward = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(690)\n"
     ]
    }
   ],
   "source": [
    "rand_inp = \"Recently I published a book of speculative nonfiction about the possibility of a civil war in the United States. For five years, I have been studying American political rage, its sources, its abyssal depths, its vertiginous fracturing power. Their forces are diminishing: About 1,000 people are left in the Ottawa convoy, although that number may rise over the weekend. Republican Senator Ted Cruz is way, way more into the trucker convoy than any Canadian Conservative is. For the first time, I felt political rage: a sharp rise in testosterone, blinding and stupefying and violent.\"\n",
    "\n",
    "dummy_inp= tokenize_sequence(rand_inp, token_basis)\n",
    "dummy_inp = dummy_inp[:600]\n",
    "dummy_inp = dummy_inp[None, :]\n",
    "\n",
    "x_lstm = dummy_inp.long()\n",
    "\n",
    "print(x_lstm[0][0])\n",
    "# model_load(recurrent, 'model_parameters/', 'lstm_politifact', export=False)\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     recurrent, \n",
    "#     x_lstm,\n",
    "#     \"lstm.onnx\",\n",
    "#     input_names=['x'],\n",
    "#     output_names=['output_lstm']\n",
    "#     # dynamic_axes = {\n",
    "#     #     \"x\": {0: \"batch_size\"}\n",
    "#     # }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "670e6febf32b15becee6ccb36535392e5e68dd97ab88bb3ce5f50b399ff4f1e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('capenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
