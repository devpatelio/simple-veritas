{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import urllib\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(DIRECTORY):\n",
    "    for name in files:\n",
    "        paths.append(os.path.join(root, name))\n",
    "\n",
    "names = [i.split('/')[-1] for i in paths][1:]\n",
    "data_dict = dict(zip([i[:-4] for i in names], paths[1:]))\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "\n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        self.max_len = max([len(i) for i in self.x_data])\n",
    "        self.max_len = 600\n",
    "\n",
    "        self.x_data, self.token = self.word_vector(self.x_data)\n",
    "\n",
    "        # list_x = self.x_data.tolist()\n",
    "        # for x, entry in enumerate(list_x):\n",
    "        #     new_entry = self.format_text(entry)\n",
    "        #     list_x[x] = new_entry\n",
    "\n",
    "        \n",
    "        # train_docs = ' '.join(map(str, list_x))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.data[x_col] = [torch.tensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        for idx, i in enumerate(x_data):\n",
    "\n",
    "            if len(i) > maximum_length:\n",
    "                maximum_length = len(i)\n",
    "                max_idx = idx\n",
    "        maximum_length = 600\n",
    "        t = Tokenizer(num_words=600, filters='\\n.,:!\"#$()&@%^()-_`~[];.,{|}')\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "\n",
    "        return sequences, t\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            data.loc[:, column] = [torch.tensor(ref[data.loc[idx, column]]) for idx in range(len(data))]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])\n",
    "token_basis = clean_truth_data.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sys.getsizeof(clean_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = np.array(next(a)[0])\n",
    "inp_size = (b.shape)[1]\n",
    "emb_dim = 6712800\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)        \n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        x = self.dropout(F.relu(self.fc7(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        print(hidden.shape)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_parameters/lstm_politifact.pth'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feedforward = FeedForward(ref_check, inp_size).to(device)\n",
    "x_f_s = next(feedforward.parameters())\n",
    "\n",
    "recurrent = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)\n",
    "x_r_s = next(recurrent.parameters())\n",
    "\n",
    "def model_load(net, PATH, name, export=True):\n",
    "    if export:\n",
    "        torch.save(net.state_dict(), PATH+name+'.pth')\n",
    "        return PATH+name+'.pth'\n",
    "    else:\n",
    "        net.torch.load_state_dict(torch.load(PATH + name + '.pth'))\n",
    "        net.eval()\n",
    "        return net\n",
    "\n",
    "model_load(feedforward, 'model_parameters/', 'linear_politifact')\n",
    "model_load(recurrent, 'model_parameters/', 'lstm_politifact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(feedforward, x_f_s, \"feedforward.onnx\")\n",
    "# torch.onnx.export(recurrent, x_r_s, \"recurrent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['David Frum', 'About The Author'],\n",
       " datetime.datetime(2022, 2, 11, 19, 1, 30, tzinfo=tzutc()),\n",
       " 'https://cdn.theatlantic.com/thumbor/vbgNQbXpLEtiIr-LOgcZgOtMSaw=/0x133:6230x3378/1200x625/media/img/mt/2022/02/GettyImages_1369598771/original.jpg',\n",
       " {\"data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 266 200' /%3E\",\n",
       "  'https://cdn.theatlantic.com/thumbor/bcKXdkaXewFxIHnPdLEdYytYgYE=/0x0:6240x3510/960x540/media/img/mt/2022/02/GettyImages_1369598771/original.jpg',\n",
       "  'https://cdn.theatlantic.com/thumbor/vbgNQbXpLEtiIr-LOgcZgOtMSaw=/0x133:6230x3378/1200x625/media/img/mt/2022/02/GettyImages_1369598771/original.jpg'},\n",
       " 'Canada’s Trucker Blockades Are a Warning',\n",
       " 'Much of the money donated in support of the Canadian protests has been raised internationally, especially in the United States.\\nThat’s the busiest of all the border crossings between the United States and Canada, crucial to moving auto parts.\\nAbout 90 percent of Canadian truck drivers are vaccinated; comparatively few of those protesting are professional truck drivers.\\nJustin Trudeau is Canada’s prime minister on the strength of 32.6 percent of the votes cast in the 2021 federal election.\\nTo a great extent, Canada still is law-abiding: When a court ordered the truck protesters to cease blaring their horns, the horn-blaring ceased.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def meta_extract(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.download('punkt')\n",
    "    article.nlp()\n",
    "    return article.authors, article.publish_date, article.top_image, article.images, article.title, article.summary\n",
    "\n",
    "meta_extract('https://www.theatlantic.com/ideas/archive/2022/02/canada-trucker-protests-spread-america/622039/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "670e6febf32b15becee6ccb36535392e5e68dd97ab88bb3ce5f50b399ff4f1e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('capenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
