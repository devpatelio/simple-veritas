{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.DS_S': 'data/truth-detectiondeception-detectionlie-detection/.DS_Store', 'politifact_clean': 'data/truth-detectiondeception-detectionlie-detection/politifact_clean.csv', 'politifact_clean_binarized': 'data/truth-detectiondeception-detectionlie-detection/politifact_clean_binarized.csv'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(DIRECTORY):\n",
    "    for name in files:\n",
    "        paths.append(os.path.join(root, name))\n",
    "\n",
    "names = [i.split('/')[-1] for i in paths][1:]\n",
    "data_dict = dict(zip([i[:-4] for i in names], paths[1:]))\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1\n",
      "tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q6/c9ls7y117c383dxbpmvcj56h0000gn/T/ipykernel_57003/2234198241.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column][idx] = vectorized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, file, root, x_col, y_col, meta_columns, label_idx = -1):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        self.data = self.data.drop(meta_columns, axis=1)\n",
    "        # print('p1')\n",
    "# \n",
    "        # self.data, self.base_ref = self.tokenizer(self.data, [x_col])\n",
    "        self.x_data = self.data[x_col]\n",
    "        # self.max_len = max([len(i) for i in self.x_data])\n",
    "        self.max_len = 600\n",
    "\n",
    "        self.x_data, self.tokenizer = self.word_vector(self.x_data)\n",
    "        # print('tokenizer')\n",
    "        self.data[x_col] = [torch.FloatTensor(i) for i in self.x_data]\n",
    "        self.data = self.vectorize(self.data, [y_col])\n",
    "        # print('vectorize')\n",
    "        self.df_data = self.data\n",
    "        self.data = self.data.to_numpy()\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "    def format_text(self, token):\n",
    "        clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "        return clean_token\n",
    "\n",
    "    def word_vector(self, data):\n",
    "        x_data = data\n",
    "        x_data = list(x_data)\n",
    "        maximum_length = 0\n",
    "        max_idx = 0\n",
    "        # for idx, i in enumerate(x_data):\n",
    "\n",
    "        #     if len(i) > maximum_length:\n",
    "        #         maximum_length = len(i)\n",
    "        #         max_idx = idx\n",
    "        \n",
    "        maximum_length = 600\n",
    "        t = Tokenizer(num_words=maximum_length)\n",
    "        t.fit_on_texts(x_data)\n",
    "        sequences = t.texts_to_sequences(x_data)\n",
    "        sequences = sequence.pad_sequences(sequences, maxlen=maximum_length)\n",
    "\n",
    "        return sequences, t\n",
    "\n",
    "\n",
    "    def vectorize(self, data_inp, columns):\n",
    "        data = data_inp\n",
    "        for column in columns:\n",
    "            labels = list(data[column].unique())\n",
    "            ref = dict(zip(data[column].unique(), [i for i in range(len(labels))]))\n",
    "            # print(ref)\n",
    "            for idx, val in enumerate(data[column]):\n",
    "                vectorized = ref[data[column][idx]]\n",
    "                data[column][idx] = vectorized\n",
    "        return data\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        self.transpose_data = self.data\n",
    "        self.transpose_data = self.transpose_data.transpose()\n",
    "        x_data = self.transpose_data[0]\n",
    "        y_data = self.transpose_data[1]\n",
    "\n",
    "        return x_data[idx], y_data[idx]\n",
    "\n",
    "clean_truth_data = PreprocessingDataset(data_dict['politifact_clean_binarized'], DIRECTORY, 'statement', 'veracity', ['source', 'link'])\n",
    "token_basis = clean_truth_data.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "primary_data = clean_truth_data #secondary option of truth_data\n",
    "\n",
    "train_len = int(len(primary_data)*0.8)\n",
    "test_len = len(primary_data) - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(primary_data, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "a = iter(train_loader)\n",
    "b = np.array(next(a)[0])\n",
    "inp_size = (b.shape)[1]\n",
    "emb_dim = 6712800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)        \n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 20)\n",
    "        self.fc7 = nn.Linear(20, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        x = self.dropout(F.relu(self.fc7(x)))\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, input_size, hidden_size, output_size, num_layers, dropout=0.3):\n",
    "        super(RecurrentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim, input_size)\n",
    "        self.rnn = nn.LSTM(input_size, \n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        print(hidden.shape)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1, :, :]), dim=1))\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "max_len = len(train_set[1][0])\n",
    "ref_check = 1\n",
    "\n",
    "feedforward = FeedForward(ref_check, inp_size).to(device)\n",
    "recurrent = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_loader, LR, DECAY, EPOCHS):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    epochs = EPOCHS\n",
    "    losses = []\n",
    "\n",
    "    for step in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inp, labels = data\n",
    "            if net == recurrent:\n",
    "                inp, labels = inp.long().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            elif net == feedforward:\n",
    "                inp, labels = inp.float().to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inp)\n",
    "                cost = loss_func(torch.squeeze(outputs), labels)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += cost.item()\n",
    "        print(f'Epoch: {step}   Training Loss: {running_loss/len(train_loader)}')\n",
    "    print('Training Complete')  \n",
    "\n",
    "    return losses\n",
    "\n",
    "def eval(net, test_loader):\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=DECAY)\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inp, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inp.float())\n",
    "        output = output.detach().numpy()\n",
    "        output = list(output)\n",
    "        output = [list(i).index(max(i)) for i in output]\n",
    "        \n",
    "        for idx, item in enumerate(torch.tensor(output)):\n",
    "            total += 1\n",
    "            if item == labels[idx]:\n",
    "                acc += 1\n",
    "    print(f'{acc/total*100}%')\n",
    "\n",
    "def format_raw_text(token):\n",
    "    token = token.replace(' ', 'uxd')\n",
    "    clean_token = ''.join(chr for chr in token if chr.isalnum() and chr.isalpha())\n",
    "    return clean_token\n",
    "\n",
    "def model_load(net, PATH, name, export=True):\n",
    "    if export:\n",
    "        torch.save(net.state_dict(), PATH+name+'.pth')\n",
    "        return PATH+name+'.pth'\n",
    "    else:\n",
    "        net.torch.load_state_dict(torch.load(PATH + name + '.pth'))\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import nltk\n",
    "\n",
    "def sentiment(inp_text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(inp_text)\n",
    "    \n",
    "def meta_extract(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.download('punkt')\n",
    "    article.nlp()\n",
    "    return article.authors, article.publish_date, article.top_image, article.images, article.title, article.summary\n",
    "\n",
    "def tokenize_sequence(text_inp, tokenizer):\n",
    "    text_inp = text_inp.lower().split('\\n')\n",
    "    tokenizer.fit_on_texts(text_inp)\n",
    "    sequences = tokenizer.texts_to_sequences(text_inp)\n",
    "    sequences = [i if i!=[] else [0] for i in tokenizer.texts_to_sequences(text_inp)]\n",
    "    sequences = [i[0] for i in sequences]\n",
    "    pad_len =  [0]*int(inp_size - len(sequences))\n",
    "    sequences += pad_len\n",
    "    return torch.FloatTensor(sequences)[:600]\n",
    "\n",
    "def prediction(inp, model):\n",
    "    output = model(inp)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flask import Flask, request, render_template, redirect, url_for\n",
    "from flask import Flask, render_template, redirect, url_for, request\n",
    "import torch.nn.functional as F\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "@app.route('/link', methods=[\"POST\", \"GET\"])\n",
    "def link():\n",
    "    if request.method == \"POST\":\n",
    "        link_inp = request.form['linker']\n",
    "        # print(type(link_inp))\n",
    "    \n",
    "        link_inp = link_inp.replace('.com', 'comkey')\n",
    "        link_inp = link_inp.replace('https://', 'https')\n",
    "        link_inp = link_inp.replace('www.', 'www')\n",
    "        link_inp = link_inp.replace('/', 'slash')\n",
    "        # print(link_inp)\n",
    "        main = link_inp\n",
    "\n",
    "        return redirect(url_for(\"preview_linker\", linkage=main, tag='link_url'))\n",
    "    else:\n",
    "        return render_template(\"link.html\")\n",
    "\n",
    "\n",
    "@app.route('/text', methods=[\"POST\", \"GET\"])\n",
    "def pure_text():\n",
    "    if request.method == \"POST\":\n",
    "        inp_raw = request.form['raw_text']\n",
    "        inp_raw = format_raw_text(inp_raw)\n",
    "        return redirect(url_for('preview_linker', linkage=inp_raw, tag='pure_text'))\n",
    "    else:\n",
    "        return render_template(\"pure_text.html\")\n",
    "\n",
    "\n",
    "@app.route(f\"/output/<tag>/<linkage>\")\n",
    "def preview_linker(linkage, tag):\n",
    "    preview = linkage\n",
    "    if tag == 'link_url':\n",
    "        preview = preview.replace('https', 'https://')\n",
    "        preview = preview.replace('www', 'www.')\n",
    "        preview = preview.replace('slash', '/')\n",
    "        preview = preview.replace('comkey', '.com')\n",
    "        authart, publ, timg, allimg, tit, summ = meta_extract(preview)\n",
    "\n",
    "    elif tag == 'pure_text':\n",
    "        preview = preview.replace('uxd', ' ')     \n",
    "        summ = preview  \n",
    "        empty_msg = 'None'\n",
    "        authart = empty_msg\n",
    "        publ = empty_msg\n",
    "        timg = empty_msg\n",
    "        allimg = empty_msg\n",
    "        tit = empty_msg\n",
    "\n",
    "    sent = sentiment(summ)\n",
    "\n",
    "    inp = tokenize_sequence(summ, token_basis)\n",
    "    inp = inp[:600]\n",
    "    inp = inp[None, :]\n",
    "    # print(inp.shape)\n",
    "\n",
    "    feedforward_template = FeedForward(ref_check, inp_size).to(device)\n",
    "    recurrent_template = RecurrentClassifier(emb_dim, inp_size, 50, ref_check, 2, dropout=0.2).to(device)    \n",
    "    model_load(feedforward_template, 'model_parameters/', 'linear_politifact')\n",
    "    model_load(recurrent_template, 'model_parameters/', 'lstm_politifact')\n",
    "\n",
    "    # feedforward_template.eval()\n",
    "    # recurrent_template.eval()\n",
    "\n",
    "    output_linear = '0 ERROR'\n",
    "    output_lstm = '1 ERROR' #check for error without passing error\n",
    "\n",
    "    output_linear = F.sigmoid(prediction(inp, feedforward_template)).round()\n",
    "    output_lstm = F.sigmoid(prediction(inp.long(), recurrent_template))\n",
    "\n",
    "    all_types = list(pd.read_csv(data_dict['politifact_clean'])['veracity'].unique())\n",
    "\n",
    "    if output_linear == 0:\n",
    "        output_linear = f\"Little Bias: Prediction = {output_linear}\"\n",
    "    elif output_linear == 1:\n",
    "        output_linear = f\"Substantial Bias: Prediction = {output_linear}\"\n",
    "\n",
    "    statement_type = ''\n",
    "    if output_lstm <= 0.25:\n",
    "        statement_type = 'True'\n",
    "    elif 0.25 < output_lstm <= 0.5:\n",
    "        statement_type = 'Mostly True'\n",
    "    elif 0.5 < output_lstm <= 0.75:\n",
    "        statement_type = 'Mostly False'\n",
    "    elif 0.75 < output_lstm <= 1:\n",
    "        statement_type = 'False'\n",
    "    elif output_lstm > 1:\n",
    "        statement_type = 'Pants on Fire!'\n",
    "\n",
    "    output_lstm = f\"Veracity -> {statement_type}: {output_lstm}\"\n",
    "\n",
    "    # if output_lstm == 0:\n",
    "    #     output_lstm = f\"Limited Veracity: Prediction = {output_lstm}\"\n",
    "    # elif output_lstm == 1:\n",
    "    #     output_lstm = f\"Expressive Veracity: Prediction = {output_lstm}\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return render_template(\"preview.html\", preview_link=preview,\n",
    "                                            author_article=authart, \n",
    "                                            published_article=publ,\n",
    "                                            top_image = timg,\n",
    "                                            all_image = allimg,\n",
    "                                            title_article=tit,\n",
    "                                            summary_article=summ,\n",
    "                                            sentiment=sent,\n",
    "                                            bias_point=output_linear,\n",
    "                                            skew_point=output_lstm)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "670e6febf32b15becee6ccb36535392e5e68dd97ab88bb3ce5f50b399ff4f1e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('capenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
